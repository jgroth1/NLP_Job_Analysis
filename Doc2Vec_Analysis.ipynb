{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, wordpunct_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paras(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for string in soup.stripped_strings:\n",
    "        yield string\n",
    "\n",
    "def sents(html):\n",
    "    for paragraph in paras(html):\n",
    "        for sentence in sent_tokenize(paragraph):\n",
    "            yield sentence\n",
    "\n",
    "def words(html):\n",
    "    for sentence in sents(html):\n",
    "        for token in wordpunct_tokenize(sentence):\n",
    "            yield token\n",
    "\n",
    "def tokenize(html):\n",
    "    for paragraph in paras(html):\n",
    "        yield [pos_tag(wordpunct_tokenize(sent)) for sent in sent_tokenize(paragraph)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "titles = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "n=0\n",
    "for i in range(10):\n",
    "    i += 1\n",
    "    path = 'corpus3/02-09-2018/Page_' + str(i) + '/'\n",
    "    files = os.listdir(path)\n",
    "    rxs = [re.search('^\\.',file) for file in files]\n",
    "    i =[n for n in range(len(rxs)) if rxs[n]]\n",
    "    m = 0\n",
    "    for k in i:\n",
    "        files.pop(k-m)\n",
    "        m += 1\n",
    "\n",
    "    for file in files: \n",
    "        with open(path + file, encoding='utf-8') as f:\n",
    "            job_post = f.read()\n",
    "\n",
    "        job_dict = eval(job_post)\n",
    "\n",
    "        job_dict.keys()\n",
    "\n",
    "        title = job_dict['job title']\n",
    "        company = job_dict['company']\n",
    "        titles.append([n, title, company])\n",
    "        html = job_dict['job description']\n",
    "        doc = []\n",
    "        xs = words(html)\n",
    "        for x in xs:\n",
    "            if x not in stop_words:\n",
    "                doc.append(wnl.lemmatize(x).lower())\n",
    "\n",
    "        docs.append(doc)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [d for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "model = Doc2Vec(documents, vector_size=50, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "n=0\n",
    "resses = []\n",
    "\n",
    "with open('/Users/grothjd/documents/DS_resume/Time_Series_Resume/Jonathan_Groth_PhD_Resume_test.txt', encoding='utf-8') as f:\n",
    "    resume = f.read()\n",
    "\n",
    "#print(resume)    \n",
    "    \n",
    "    \n",
    "xs = words(resume)\n",
    "for x in xs:\n",
    "    if x not in stop_words:\n",
    "        resses.append(wnl.lemmatize(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/PythonData/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = model.infer_vector(resses)\n",
    "sims = model.docvecs.most_similar([inferred_vector])\n",
    "\n",
    "select = []\n",
    "for sim in sims:\n",
    "    select.append(sim[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 'Data Scientist', 'Envision'], [26, 'Data Scientist', 'Aptima'], [28, 'Data Scientist', 'Enterprise Solution inc'], [35, 'Data Scientist', 'Jobspring Partners'], [47, 'Data Scientist', 'Collabera'], [51, 'Data Scientist', 'Hertz'], [151, 'Research Data Scientist, Geospatial Analytics', 'Iconma'], [222, 'BT-1582 Computer &amp; Information Research Scientist', 'Bastion Technologies'], [229, 'Data Scientistâž', '3m'], [248, 'Lead Data Eng-Demand Sensing, Enterprise Data', 'Nike']]\n"
     ]
    }
   ],
   "source": [
    "job = []\n",
    "for title in titles:\n",
    "    if int(title[0]) in select:\n",
    "        job.append(title)\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/PythonData/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "keywords = model.wv.similar_by_word(\"data\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('analytics', 0.9885436296463013),\n",
       " ('mining', 0.9879107475280762),\n",
       " ('analysis', 0.9867527484893799),\n",
       " ('predictive', 0.9866224527359009),\n",
       " ('statistical', 0.9833877682685852),\n",
       " ('advanced', 0.9829444885253906),\n",
       " ('modeling', 0.9804550409317017),\n",
       " ('tool', 0.980415403842926),\n",
       " ('technique', 0.9800536632537842),\n",
       " ('algorithm', 0.97920823097229),\n",
       " ('knowledge', 0.9750123620033264),\n",
       " ('model', 0.969063401222229),\n",
       " ('using', 0.9687738418579102),\n",
       " (';', 0.9671470522880554),\n",
       " ('deep', 0.9654371738433838),\n",
       " ('method', 0.9609599113464355),\n",
       " ('software', 0.9607940316200256),\n",
       " ('big', 0.9573837518692017),\n",
       " ('statistic', 0.9543017745018005),\n",
       " ('unsupervised', 0.953468382358551),\n",
       " ('analytic', 0.9528532028198242),\n",
       " ('programming', 0.9516748189926147),\n",
       " ('modelling', 0.9495691061019897),\n",
       " ('analytical', 0.948169469833374),\n",
       " ('simulation', 0.9477257132530212),\n",
       " ('segmentation', 0.946719765663147),\n",
       " ('computational', 0.9429284334182739),\n",
       " ('design', 0.9419755339622498),\n",
       " ('visualization', 0.941131591796875),\n",
       " ('linear', 0.9407906532287598),\n",
       " ('profiling', 0.9405220746994019),\n",
       " ('artificial', 0.938550591468811),\n",
       " ('supervised', 0.937903881072998),\n",
       " ('development', 0.9378916621208191),\n",
       " ('architecture', 0.9372762441635132),\n",
       " ('mathematical', 0.9370237588882446),\n",
       " ('leveraging', 0.9368352293968201),\n",
       " ('tuning', 0.9363853335380554),\n",
       " ('r', 0.9345619082450867),\n",
       " ('understanding', 0.933614194393158),\n",
       " ('language', 0.9330748915672302),\n",
       " ('sql', 0.9325013756752014),\n",
       " ('optimization', 0.9320335984230042),\n",
       " ('sas', 0.9319844841957092),\n",
       " ('coursework', 0.9317729473114014),\n",
       " ('strong', 0.9311676025390625),\n",
       " ('applied', 0.9307022094726562),\n",
       " ('wrangling', 0.9301570653915405),\n",
       " ('deploying', 0.9296387434005737),\n",
       " ('warehousing', 0.929211437702179),\n",
       " ('large', 0.9291297197341919),\n",
       " ('regression', 0.928142249584198),\n",
       " ('python', 0.9277464151382446),\n",
       " ('working', 0.9267599582672119),\n",
       " ('ensemble', 0.9263086318969727),\n",
       " ('cybersecurity', 0.9262212514877319),\n",
       " ('researching', 0.9252607226371765),\n",
       " ('relational', 0.9247971177101135),\n",
       " ('experimental', 0.9239134788513184),\n",
       " ('methodology', 0.9238977432250977),\n",
       " ('quantitatively', 0.9238930344581604),\n",
       " ('quantitative', 0.9231771230697632),\n",
       " ('logistic', 0.922852635383606),\n",
       " ('·-', 0.9216709733009338),\n",
       " ('processing', 0.9215505123138428),\n",
       " ('develop', 0.9202592372894287),\n",
       " ('ai', 0.9196290969848633),\n",
       " ('numerical', 0.9191699028015137),\n",
       " ('descriptive', 0.919022798538208),\n",
       " ('compiling', 0.9183995127677917),\n",
       " ('set', 0.9183788299560547),\n",
       " ('biostatistics', 0.9174140691757202),\n",
       " ('expert', 0.9171024560928345),\n",
       " ('attained', 0.9170273542404175),\n",
       " ('specialization', 0.9168438911437988),\n",
       " ('management', 0.9165505170822144),\n",
       " ('implement', 0.91654372215271),\n",
       " ('popular', 0.9158748984336853),\n",
       " ('signal', 0.9154074788093567),\n",
       " ('text', 0.9147197008132935),\n",
       " ('nlp', 0.913974940776825),\n",
       " ('ex', 0.913809597492218),\n",
       " ('utilizes', 0.9137570858001709),\n",
       " ('churn', 0.9130145311355591),\n",
       " ('training', 0.911184549331665),\n",
       " ('database', 0.910568118095398),\n",
       " ('linguistics', 0.910545289516449),\n",
       " ('determines', 0.9099467992782593),\n",
       " ('relevant', 0.9098832607269287),\n",
       " ('solid', 0.909241795539856),\n",
       " ('•', 0.909124493598938),\n",
       " ('shell', 0.9088079333305359),\n",
       " ('learning', 0.9079120755195618),\n",
       " ('dimensional', 0.9074375033378601),\n",
       " ('warehouse', 0.907175600528717),\n",
       " ('extensive', 0.9071037769317627),\n",
       " ('technology', 0.9069817662239075),\n",
       " ('hadoop', 0.906354546546936),\n",
       " ('applying', 0.9063044190406799),\n",
       " ('svm', 0.9061485528945923)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
