{"job title": "Data Engineer", "company": "Segment", "city state": "\u2013 San Francisco, CA", "rating": "4.8", "job description": "<div>\n<strong>Overview</strong></div><br>\n\n<div>At Segment, we believe companies should be able to send their data wherever they want, whenever they want, with no fuss. Unfortunately, most product managers, analysts, and marketers spend too much time searching for the data they need, while engineers are stuck in integrating the tools they want to use. Segment standardizes and streamlines data infrastructure with a single platform that collects, unifies, and sends data to hundreds of business tools with the flip of a switch. That way, our customers can focus on building amazing products and personalized messages for their customers, letting us take care of the complexities of processing their customer data reliably at scale. We're in the running to power the entire customer data ecosystem, and we need the best people to take the market.</div>\n<br>\n<div>Data Engineering enables Segment to derive insights about our customers and product usage effectively and efficiently, and it is the backbone of all data-driven decisions we make to move the business forward.</div>\n<br>\n<div>This is a rare opportunity to be one of the founding members of Segment's internal Data Engineering team. As a founding data engineer, you will be part of the core team responsible for building out our data lake, processing many terabytes of data, and developing efficient ETL pipelines to deliver data and insights to partners across the company. Additionally, you have the opportunity to provide feedback to Product and Engineering teams that will help shape the future of our products.</div>\n<br>\n<strong><div>\nWhat you'll do:</div>\n\n<br></strong>\n\n<ul>\n\n\n\n<ul>\n<li>Design, build and launch efficient and reliable data pipelines</li>\n<li>Work with partners including Analytics, Product, and Operations teams to assist with their data infrastructure needs</li>\n<li>Optimize Segment's internal data storage and compute resources to improve performance, reliability, and availability of the data</li>\n<li>Execute on our data lake strategy</li>\n<li>Own and maintain Segment's internal data infrastructure assets primarily hosted on AWS</li>\n<li>Build backend data services for internal applications teams to consume</li>\n<li>Build tools and frameworks, such as schema management and evolution and data quality tools, to facilitate efficient and reliable data processing</li></ul>\n\n\n</ul>\n\n<div>\n<strong>You're a great fit if you have</strong></div>\n\n<ul>\n\n\n\n<ul>\n<li>1+ years of hands-on experience with Python, Java, or Scala for data processing</li>\n<li>1+ years of experience building big data (terabyte scale) processing pipelines using a distributed data processing engine/framework</li>\n<li>1+ years of working experience with analytical databases, such as Redshift, as well as database and query optimization</li>\n<li>3+ years of data or software engineering experience</li>\n<li>3+ years of advanced working SQL experience</li>\n<li>3+ years of working experience with relational SQL and NoSQL databases such as Postgres, MySQL, and MongoDB</li>\n<li>Hands-on experience with versioning, continuous integration, and build and deployment tools and platforms such as Github and Circle CI</li>\n<li>Knowledge of data pipelining and workflow management tools such as Airflow, AWS Data Pipelines, and Luigi</li>\n<li>Familiar with dimensional data modeling and data normalization</li>\n<li>Familiar with data-lake architecture, and data serialization formats such as JSON, Avro, and Parquet</li>\n<li>You are eager and willing to learn and work in a dynamic and fast-paced shop</li>\n<li>You have strong interpersonal skills</li>\n<li>You have a BS or MS degree in Computer Science or a related technical field</li></ul>\n\n\n</ul>\n\n<strong><div>\nBonus points:</div>\n\n<br></strong>\n\n<ul>\n\n\n\n<ul>\n<li>Experience with AWS cloud services: EC2, EMR Spark, EMR Hive, RDS, Redshift, Athena, Spectrum, and AWS Glue</li>\n<li>Hands-on experiences with any of these technologies and platforms: Hive SQL, Hive Meta-Store, PySpark, Spark Streaming, Athena, Redshift Spectrum, AWS Glue Data Catalog, Spark Streaming, Kafka, Kinesis, Spark Streaming, and Terraform</li>\n<li>Hands-on experience with data stream processing</li></ul>\n\n\n</ul>\n\n<div>This role requires being <strong>full-time</strong> in our San Francisco office</div>\n<br>\n<div>Segment is an equal opportunity employer. We believe that everyone should receive equal consideration and treatment. Recruitment, hiring, placements, transfers, and promotions will happen based on qualifications for the positions being filled regardless of sex, gender identity, race, religious creed, color, national origin ancestry, age, physical disability, pregnancy, mental disability, or medical condition.</div>"}