{"job title": "Senior Data Engineer", "company": "KAR Auction Services", "city state": "\u2013 Chicago, IL", "rating": "3.0", "job description": "We are fast-paced, forward thinking and driven by data.\n<br><br>\nWe are accelerating the used car industry.\n<br><br>\nWe are looking for creative, talented, hard-working individuals to join us.\n<br><br>\nBuckle up. It's going to be a great ride.\n<br><br>\nBased in Chicago, DRIVIN is accelerating the used car industry by bringing data and technology together in a spectacular, first-of-its-kind fashion to help dealers acquire the right cars, at the right price, right now. We are committed to delivering data-driven solutions with a high-touch, personalized level of service to each of our clients. We are looking for people that will help us create a culture that is exciting, empowering, motivating and challenging. Are you interested in learning more?\n\n\n<ul>\n<li>DRIVIN is looking to expand our data team as we continue to grow our data platform. The candidate should have a strong background with Python and SQL. As a member of the data team the main responsibilities are implementing/maintaining ETL jobs, using Python to ingest external data sources into the Data Warehouse, and working closely with the Product and Data Science teams to deliver data in useable formats and to the appropriate data sources.</li>\n</ul>\n\nDRIVIN has a polyglot data model using many cutting-edge data platforms. We are currently using MPP Postgres (Greenplum, Netezza, DBX) as our Data Warehouse, Elastic Search for location based searching, Postgres for transactional data.\n<br><br>\nThis candidate should be a self-starter who is interested in learning new systems/environments and building new solutions.\n<br><br>\nThe candidate should also work closely with the Data Science team to identify interesting data points for use by the Data Science team.\n<br><br>\nDRIVIN tech stack is very cutting edge. MPP Postgres drives the Data Warehouse, ElasticSearch enables our location based searching/metrics, and Apache Spark is used to train our models. All environments are run on AWS EC2/RDS/S3 and data processing framework is written in Python.\n<br><br>\n<strong>RESPONSIBILITIES:</strong>\n\n\n<ul>\n<li>Implement ETL jobs for various functions</li>\n<li>Support and maintain daily ETL jobs</li>\n<li>Support the development teams by optimizing data access</li>\n<li>Work with data science teams to deliver metrics to consumers</li>\n</ul>\n\n<strong>RESPONSIBILITIES:</strong>\n\n\n<ul>\n<li>Implement ETL jobs for various functions</li>\n<li>Support and maintain daily ETL jobs</li>\n<li>Support the development teams by optimizing data access</li>\n<li>Work with data science teams to deliver metrics to consumers</li>\n</ul>"}